{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCE Calculations of Testing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ASCVD_Calc_PCE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from importlib import reload\n",
    "reload(ASCVD_Calc_PCE)\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, RocCurveDisplay, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Volumes/fsmresfiles/PrevMed/Projects/MESA_RiskPred/LRPP data/LRPP_select.csv')\n",
    "indecies_test = pd.read_csv('/Volumes/fsmresfiles/PrevMed/Projects/MESA_RiskPred/LRPP data/random_index_select.csv')\n",
    "\n",
    "indecies_test.columns = ['index', 'label', 'study_index']\n",
    "\n",
    "indecies_test = indecies_test.loc[indecies_test['study_index'] == False, :]\n",
    "df = df.loc[df['id'].isin(indecies_test['index']), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get labels and tte\n",
    "te_label = np.array(df.loc[:,['id', 'label']].drop_duplicates().label)\n",
    "te_time = np.array(df.loc[:, ['id', 'tte']].drop_duplicates().tte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCE Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get risk measurements from PCE data set (Multiple)\n",
    "pce_pred_df, pce_df = ASCVD_Calc_PCE.pce_pred_df_tab(df, 8, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate AUC \n",
    "def prediction_auc_PCE_df(pred_time, pred_time_index, pce_pred_df, cluster):\n",
    "    \"\"\"\n",
    "    Calculates the AUC of the PCE predictions at a given time\n",
    "    pred_time: pred time list of prediction times\n",
    "    pred_time_index: which prediction time in terms of position in the list \n",
    "    pce_pred_df: df of probability of PCE dataset\n",
    "    \"\"\"\n",
    "    global true, time_horizon\n",
    "    \n",
    "    time_horizon = pred_time + 10 \n",
    "    true = (te_time <= time_horizon) * (te_label == 1).astype(int)\n",
    "    \n",
    "    pce_pred_df['true_label'] = true\n",
    "    pce_pred_df = pce_pred_df.loc[~pce_pred_df.risk.isnull(),:]\n",
    "    \n",
    "    if cluster != 'none':\n",
    "        pce_pred_df = pce_pred_df.loc[pce_pred_df['labels'] == cluster, :]\n",
    "        \n",
    "    return pce_pred_df\n",
    "\n",
    "def prediction_auc_PCE(pred_time, pred_time_index, pce_pred_df, cluster):\n",
    "    \n",
    "    pce_pred_df = prediction_auc_PCE_df(pred_time, pred_time_index, pce_pred_df, cluster)\n",
    "    auc = roc_auc_score(pce_pred_df['true_label'].tolist(), pce_pred_df['risk'].tolist())\n",
    "    \n",
    "    return auc \n",
    "\n",
    "def prediction_ROC(pred_time, pred_time_index, pce_pred_df, cluster):\n",
    "    \n",
    "    pce_pred_df = prediction_auc_PCE_df(pred_time, pred_time_index, pce_pred_df, cluster)\n",
    "    fpr, tpr, thresh = roc_curve(pce_pred_df['true_label'].tolist(), pce_pred_df['risk'].tolist())\n",
    "    \n",
    "    return fpr, tpr, thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NULL Risk Values from PCE:  0\n",
      "PCE AUC: 0.801\n"
     ]
    }
   ],
   "source": [
    "# Get PCE AUC for Each Prediction Year\n",
    "print('NULL Risk Values from PCE: ', pce_pred_df.risk.isna().sum())\n",
    "print('PCE AUC:', np.round(prediction_auc_PCE(8, 10, pce_pred_df, 'none'),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Risk Categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Risk Categories\n",
    "1. Low: < 5% \n",
    "2. Borderline: 5% ~ 7.5% \n",
    "3. Intermediate: 7.5% ~ 20%\n",
    "4. High: > 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pce_pred_df['risk_cat'] = 'Low Risk'\n",
    "pce_pred_df.loc[(pce_pred_df['risk'] >= 0.05) & (pce_pred_df['risk'] < 0.075), 'risk_cat'] = 'Borderline Risk'\n",
    "pce_pred_df.loc[(pce_pred_df['risk'] >= 0.075) & (pce_pred_df['risk'] < 0.2), 'risk_cat'] = 'Intermediate Risk'\n",
    "pce_pred_df.loc[(pce_pred_df['risk'] >= 0.2), 'risk_cat'] = 'High Risk'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DDH Risk Breakdown by PCE Standards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Low Risk             47.7\n",
       "Intermediate Risk    28.6\n",
       "Borderline Risk      13.9\n",
       "High Risk             9.9\n",
       "Name: risk_cat, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(pce_pred_df.risk_cat.value_counts() / len(pce_pred_df),3)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>risk_cat</th>\n",
       "      <th>true_label</th>\n",
       "      <th>counts</th>\n",
       "      <th>total_counts</th>\n",
       "      <th>percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Borderline Risk</td>\n",
       "      <td>0</td>\n",
       "      <td>1659</td>\n",
       "      <td>1727</td>\n",
       "      <td>96.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Borderline Risk</td>\n",
       "      <td>1</td>\n",
       "      <td>68</td>\n",
       "      <td>1727</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>High Risk</td>\n",
       "      <td>0</td>\n",
       "      <td>980</td>\n",
       "      <td>1228</td>\n",
       "      <td>79.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>High Risk</td>\n",
       "      <td>1</td>\n",
       "      <td>248</td>\n",
       "      <td>1228</td>\n",
       "      <td>20.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Intermediate Risk</td>\n",
       "      <td>0</td>\n",
       "      <td>3216</td>\n",
       "      <td>3557</td>\n",
       "      <td>90.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Intermediate Risk</td>\n",
       "      <td>1</td>\n",
       "      <td>341</td>\n",
       "      <td>3557</td>\n",
       "      <td>9.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Low Risk</td>\n",
       "      <td>0</td>\n",
       "      <td>5874</td>\n",
       "      <td>5940</td>\n",
       "      <td>98.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Low Risk</td>\n",
       "      <td>1</td>\n",
       "      <td>66</td>\n",
       "      <td>5940</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            risk_cat  true_label  counts  total_counts  percentage\n",
       "0    Borderline Risk           0    1659          1727        96.1\n",
       "1    Borderline Risk           1      68          1727         3.9\n",
       "2          High Risk           0     980          1228        79.8\n",
       "3          High Risk           1     248          1228        20.2\n",
       "4  Intermediate Risk           0    3216          3557        90.4\n",
       "5  Intermediate Risk           1     341          3557         9.6\n",
       "6           Low Risk           0    5874          5940        98.9\n",
       "7           Low Risk           1      66          5940         1.1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.DataFrame(pce_pred_df.groupby('risk_cat').true_label.value_counts()).rename(columns = {'true_label' : 'counts'}).reset_index()\n",
    "test2 = pd.DataFrame(pce_pred_df.risk_cat.value_counts()).reset_index().rename(columns = {'risk_cat' : 'total_counts', 'index' : 'risk_cat'})\n",
    "test = pd.merge(test, test2)\n",
    "test['percentage'] = round(test['counts'] / test['total_counts'],3) * 100\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pce_pred_df.to_csv('pce_pred_training_df.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "-----------\n",
    "## Compare Risk Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddh_pred_df = pd.read_csv('/Users/excenity/Dropbox/HSIP/Research/MESA/Output/DDH_pred_df.csv')\n",
    "\n",
    "ddh_pred_df = ddh_pred_df.loc[:, ['pt_id', 'value']].rename(columns = {'value' : 'ddh_risk'})\n",
    "\n",
    "pred_df = pd.merge(ddh_pred_df, pce_pred_df.loc[:,['risk', 'true_label', 'risk_cat']].reset_index().rename(columns = {'index' : 'pt_id'})).rename(columns = {'risk' : 'pce_risk'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean PCE Risk\n",
    "round(pred_df.groupby('risk_cat').pce_risk.mean(),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean DDH Risk\n",
    "round(pred_df.groupby('risk_cat').ddh_risk.mean(),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCE AUC by Risk Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get accuracy scores for each risk category\n",
    "print(round(roc_auc_score(pred_df.loc[pred_df['risk_cat'] == 'Low Risk', 'true_label'], pred_df.loc[pred_df['risk_cat'] == 'Low Risk', 'pce_risk']), 3))\n",
    "print(round(roc_auc_score(pred_df.loc[pred_df['risk_cat'] == 'Borderline Risk', 'true_label'], pred_df.loc[pred_df['risk_cat'] == 'Borderline Risk', 'pce_risk']), 3))\n",
    "print(round(roc_auc_score(pred_df.loc[pred_df['risk_cat'] == 'Intermediate Risk', 'true_label'], pred_df.loc[pred_df['risk_cat'] == 'Intermediate Risk', 'pce_risk']),3))\n",
    "print(round(roc_auc_score(pred_df.loc[pred_df['risk_cat'] == 'High Risk', 'true_label'], pred_df.loc[pred_df['risk_cat'] == 'High Risk', 'pce_risk']), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DDH AUC by Risk Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get accuracy scores for each risk category\n",
    "print(round(roc_auc_score(pred_df.loc[pred_df['risk_cat'] == 'Low Risk', 'true_label'], pred_df.loc[pred_df['risk_cat'] == 'Low Risk', 'ddh_risk']), 3))\n",
    "print(round(roc_auc_score(pred_df.loc[pred_df['risk_cat'] == 'Borderline Risk', 'true_label'], pred_df.loc[pred_df['risk_cat'] == 'Borderline Risk', 'ddh_risk']), 3))\n",
    "print(round(roc_auc_score(pred_df.loc[pred_df['risk_cat'] == 'Intermediate Risk', 'true_label'], pred_df.loc[pred_df['risk_cat'] == 'Intermediate Risk', 'ddh_risk']),3))\n",
    "print(round(roc_auc_score(pred_df.loc[pred_df['risk_cat'] == 'High Risk', 'true_label'], pred_df.loc[pred_df['risk_cat'] == 'High Risk', 'ddh_risk']), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RocCurveDisplay.from_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
