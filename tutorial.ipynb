{"cells":[{"cell_type":"markdown","metadata":{"id":"wsLq8iXIk5s-"},"source":["# Dynamic-DeepHit Tutorial\n","\n","### by Changhee Lee"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29350,"status":"ok","timestamp":1676320905943,"user":{"displayName":"Kevin Yu","userId":"12965521010022133262"},"user_tz":360},"id":"pPUFeura9V-i","outputId":"36436969-44c8-458b-f535-20b0d2b474b3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting lifelines\n","  Downloading lifelines-0.27.4-py3-none-any.whl (349 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m349.7/349.7 KB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.8/dist-packages (from lifelines) (1.21.6)\n","Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.8/dist-packages (from lifelines) (3.2.2)\n","Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from lifelines) (1.7.3)\n","Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from lifelines) (1.3.5)\n","Collecting autograd-gamma>=0.3\n","  Downloading autograd-gamma-0.5.0.tar.gz (4.0 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: autograd>=1.5 in /usr/local/lib/python3.8/dist-packages (from lifelines) (1.5)\n","Collecting formulaic>=0.2.2\n","  Downloading formulaic-0.5.2-py3-none-any.whl (77 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 KB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.8/dist-packages (from autograd>=1.5->lifelines) (0.16.0)\n","Requirement already satisfied: wrapt>=1.0 in /usr/local/lib/python3.8/dist-packages (from formulaic>=0.2.2->lifelines) (1.14.1)\n","Requirement already satisfied: astor>=0.8 in /usr/local/lib/python3.8/dist-packages (from formulaic>=0.2.2->lifelines) (0.8.1)\n","Collecting graphlib-backport>=1.0.0\n","  Downloading graphlib_backport-1.0.3-py3-none-any.whl (5.1 kB)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from formulaic>=0.2.2->lifelines) (4.4.0)\n","Collecting interface-meta>=1.2.0\n","  Downloading interface_meta-1.3.0-py3-none-any.whl (14 kB)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.0->lifelines) (1.4.4)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.0->lifelines) (2.8.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.0->lifelines) (0.11.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.0->lifelines) (3.0.9)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.0.0->lifelines) (2022.7.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib>=3.0->lifelines) (1.15.0)\n","Building wheels for collected packages: autograd-gamma\n","  Building wheel for autograd-gamma (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for autograd-gamma: filename=autograd_gamma-0.5.0-py3-none-any.whl size=4048 sha256=343dfcb066d95cf1660d3e3f8bf9b7291d0b3109ab1b49c48c62c204e0c78b0f\n","  Stored in directory: /root/.cache/pip/wheels/16/a2/b6/582cfdfbeeccd469504a01af3bb952fd9e7eccba40995eafea\n","Successfully built autograd-gamma\n","Installing collected packages: interface-meta, graphlib-backport, autograd-gamma, formulaic, lifelines\n","Successfully installed autograd-gamma-0.5.0 formulaic-0.5.2 graphlib-backport-1.0.3 interface-meta-1.3.0 lifelines-0.27.4\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting numpy==1.18.5\n","  Downloading numpy-1.18.5-cp38-cp38-manylinux1_x86_64.whl (20.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.6/20.6 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: numpy\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.21.6\n","    Uninstalling numpy-1.21.6:\n","      Successfully uninstalled numpy-1.21.6\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","xarray 2022.12.0 requires numpy>=1.20, but you have numpy 1.18.5 which is incompatible.\n","xarray-einstats 0.5.1 requires numpy>=1.20, but you have numpy 1.18.5 which is incompatible.\n","tensorflow 2.11.0 requires numpy>=1.20, but you have numpy 1.18.5 which is incompatible.\n","tables 3.7.0 requires numpy>=1.19.0, but you have numpy 1.18.5 which is incompatible.\n","plotnine 0.8.0 requires numpy>=1.19.0, but you have numpy 1.18.5 which is incompatible.\n","jaxlib 0.3.25+cuda11.cudnn805 requires numpy>=1.20, but you have numpy 1.18.5 which is incompatible.\n","jax 0.3.25 requires numpy>=1.20, but you have numpy 1.18.5 which is incompatible.\n","cupy-cuda11x 11.0.0 requires numpy<1.26,>=1.20, but you have numpy 1.18.5 which is incompatible.\n","cmdstanpy 1.1.0 requires numpy>=1.21, but you have numpy 1.18.5 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed numpy-1.18.5\n"]}],"source":["!pip install lifelines \n","!pip install -U numpy==1.18.5"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":416},"executionInfo":{"elapsed":1623,"status":"error","timestamp":1676321099610,"user":{"displayName":"Kevin Yu","userId":"12965521010022133262"},"user_tz":360},"id":"I_AkLpsslJ_Z","outputId":"cee0113b-f840-4163-dea2-c3c1c9d0894f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-d0d5d09f6a5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'/content/drive/My Drive/Longitudinal Risk Prediction/Dynamic-DeepHit LRPP'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tensorflow_version'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'1.x'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2312\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2313\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2314\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2315\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/_tensorflow_magics.py\u001b[0m in \u001b[0;36m_tensorflow_version\u001b[0;34m(line)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     raise ValueError(\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;31m# pylint: disable=line-too-long\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         textwrap.dedent(\"\"\"\\\n","\u001b[0;31mValueError\u001b[0m: Tensorflow 1 is unsupported in Colab.\n\nYour notebook should be updated to use Tensorflow 2.\nSee the guide at https://www.tensorflow.org/guide/migrate#migrate-from-tensorflow-1x-to-tensorflow-2."]}],"source":["# Mount your google drive in google colab\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount= True)\n","\n","# Insert the directory\n","import sys\n","sys.path.insert(0,'/content/drive/My Drive/Longitudinal Risk Prediction/Dynamic-DeepHit LRPP')\n","\n","%tensorflow_version 1.x"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":496},"executionInfo":{"elapsed":4619,"status":"error","timestamp":1676321064210,"user":{"displayName":"Kevin Yu","userId":"12965521010022133262"},"user_tz":360},"id":"Z88bYYDhk5tA","outputId":"f76238b1-0750-4548-e877-6fa83bd5694d"},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-133c3bbb8d6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#import AUC_calculator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mclass_DeepLongitudinal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel_Longitudinal_Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils_eval\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mc_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbrier_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/My Drive/Longitudinal Risk Prediction/Dynamic-DeepHit LRPP/class_DeepLongitudinal.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfully_connected\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mFC_Net\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_transpose_batch_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.contrib'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["_EPSILON = 1e-08\n","\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import random\n","import os\n","import warnings\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import roc_auc_score, roc_curve\n","\n","import matplotlib.pyplot as plt\n","\n","import import_data as impt\n","#import AUC_calculator\n","\n","from class_DeepLongitudinal import Model_Longitudinal_Attention\n","\n","from utils_eval             import c_index, brier_score\n","from utils_log              import save_logging, load_logging\n","from utils_helper           import f_get_minibatch, f_get_boosted_trainset\n","\n","warnings.filterwarnings('ignore')\n","\n","import importlib\n","\n","importlib.reload(impt)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"aborted","timestamp":1676320922355,"user":{"displayName":"Kevin Yu","userId":"12965521010022133262"},"user_tz":360},"id":"rQnB1_2oAOve"},"outputs":[],"source":["!nvidia-smi\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"aborted","timestamp":1676320922355,"user":{"displayName":"Kevin Yu","userId":"12965521010022133262"},"user_tz":360},"id":"dTltebB6k5tB"},"outputs":[],"source":["def _f_get_pred(sess, model, data, data_mi, pred_horizon):\n","    '''\n","        predictions based on the prediction time.\n","        create new_data and new_mask2 that are available previous or equal to the prediction time (no future measurements are used)\n","    '''\n","    new_data    = np.zeros(np.shape(data))\n","    new_data_mi = np.zeros(np.shape(data_mi))\n","\n","    meas_time = np.concatenate([np.zeros([np.shape(data)[0], 1]), np.cumsum(data[:, :, 0], axis=1)[:, :-1]], axis=1)\n","\n","    for i in range(np.shape(data)[0]):\n","        last_meas = np.sum(meas_time[i, :] <= pred_horizon)\n","\n","        new_data[i, :last_meas, :]    = data[i, :last_meas, :]\n","        new_data_mi[i, :last_meas, :] = data_mi[i, :last_meas, :]\n","\n","    return model.predict(new_data, new_data_mi)\n","\n","\n","def f_get_risk_predictions(sess, model, data_, data_mi_, pred_time, eval_time):\n","    \n","    pred = _f_get_pred(sess, model, data_[[0]], data_mi_[[0]], 0)\n","    _, num_Event, num_Category = np.shape(pred)\n","       \n","    risk_all = {}\n","    for k in range(num_Event):\n","        risk_all[k] = np.zeros([np.shape(data_)[0], len(pred_time), len(eval_time)])\n","            \n","    for p, p_time in enumerate(pred_time):\n","        ### PREDICTION\n","        pred_horizon = int(p_time)\n","        pred = _f_get_pred(sess, model, data_, data_mi_, pred_horizon)\n","\n","\n","        for t, t_time in enumerate(eval_time):\n","            eval_horizon = int(t_time) + pred_horizon #if eval_horizon >= num_Category, output the maximum...\n","\n","            # calculate F(t | x, Y, t >= t_M) = \\sum_{t_M <= \\tau < t} P(\\tau | x, Y, \\tau > t_M)\n","            risk = np.sum(pred[:,:,pred_horizon:(eval_horizon+1)], axis=2) #risk score until eval_time\n","            risk = risk / (np.sum(np.sum(pred[:,:,pred_horizon:], axis=2), axis=1, keepdims=True) +_EPSILON) #conditioniong on t > t_pred\n","            \n","            for k in range(num_Event):\n","                risk_all[k][:, p, t] = risk[:, k]\n","                \n","    return risk_all"]},{"cell_type":"markdown","metadata":{"id":"QCuApfBOk5tC"},"source":["### 1. Import Dataset\n","#####      - Users must prepare dataset in csv format and modify 'import_data.py' following our examplar 'PBC2'"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1676320922356,"user":{"displayName":"Kevin Yu","userId":"12965521010022133262"},"user_tz":360},"id":"d1__3nF9k5tD"},"outputs":[],"source":["data_mode                   = 'PBC2' \n","seed                        = 1234\n","\n","##### IMPORT DATASET\n","'''\n","    num_Category            = max event/censoring time * 1.2\n","    num_Event               = number of evetns i.e. len(np.unique(label))-1\n","    max_length              = maximum number of measurements\n","    x_dim                   = data dimension including delta (1 + num_features)\n","    x_dim_cont              = dim of continuous features\n","    x_dim_bin               = dim of binary features\n","    mask1, mask2, mask3     = used for cause-specific network (FCNet structure)\n","'''\n","\n","if data_mode == 'PBC2':\n","    (x_dim, x_dim_cont, x_dim_bin), (data, time, label), (mask1, mask2, mask3), (data_mi), df_raw = impt.import_dataset(norm_mode = 'standard')\n","    \n","    \n","    # This must be changed depending on the datasets, prediction/evaliation times of interest\n","    pred_time = [8] # prediction time (in years)\n","    eval_time = [10] # years evaluation time (for C-index and Brier-Score)\n","else:\n","    print ('ERROR:  DATA_MODE NOT FOUND !!!')\n","\n","_, num_Event, num_Category  = np.shape(mask1)  # dim of mask3: [subj, Num_Event, Num_Category]\n","max_length                  = np.shape(data)[1]\n","\n","\n","file_path = '{}'.format(data_mode)\n","\n","if not os.path.exists(file_path):\n","    os.makedirs(file_path)"]},{"cell_type":"markdown","metadata":{"id":"L-xKLHkwk5tD"},"source":["### 2. Set Hyper-Parameters\n","##### - Play with your own hyper-parameters!"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1676320922356,"user":{"displayName":"Kevin Yu","userId":"12965521010022133262"},"user_tz":360},"id":"6_GR0aoOk5tE"},"outputs":[],"source":["burn_in_mode                = 'OFF' #{'ON', 'OFF'}\n","boost_mode                  = 'OFF' #{'ON', 'OFF'}\n","\n","##### HYPER-PARAMETERS\n","new_parser = {'mb_size': 128,\n","\n","             'iteration_burn_in': 3000,\n","             'iteration': 10000,\n","\n","             'keep_prob': 1,\n","             'lr_train': 5e-3,\n","\n","             'h_dim_RNN': 50,\n","             'h_dim_FC' : 100,\n","             'num_layers_RNN':5,\n","             'num_layers_ATT':2,\n","             'num_layers_CS' :4,\n","\n","             'RNN_type':'LSTM', #{'LSTM', 'GRU'}\n","\n","             'FC_active_fn' : tf.nn.relu,\n","             'RNN_active_fn': tf.nn.tanh,\n","\n","            'reg_W'         : 1e-05,\n","            'reg_W_out'     : 1e-05,\n","\n","             'alpha' :2.0,\n","             'beta'  :0.0,\n","             'gamma' :1.0\n","}\n","\n","\n","# INPUT DIMENSIONS\n","input_dims                  = { 'x_dim'         : x_dim,\n","                                'x_dim_cont'    : x_dim_cont,\n","                                'x_dim_bin'     : x_dim_bin,\n","                                'num_Event'     : num_Event,\n","                                'num_Category'  : num_Category,\n","                                'max_length'    : max_length }\n","\n","# NETWORK HYPER-PARMETERS\n","network_settings            = { 'h_dim_RNN'         : new_parser['h_dim_RNN'],\n","                                'h_dim_FC'          : new_parser['h_dim_FC'],\n","                                'num_layers_RNN'    : new_parser['num_layers_RNN'],\n","                                'num_layers_ATT'    : new_parser['num_layers_ATT'],\n","                                'num_layers_CS'     : new_parser['num_layers_CS'],\n","                                'RNN_type'          : new_parser['RNN_type'],\n","                                'FC_active_fn'      : new_parser['FC_active_fn'],\n","                                'RNN_active_fn'     : new_parser['RNN_active_fn'],\n","                                'initial_W'         : tf.contrib.layers.xavier_initializer(),\n","\n","                                'reg_W'             : new_parser['reg_W'],\n","                                'reg_W_out'         : new_parser['reg_W_out']\n","                                 }\n","\n","\n","mb_size           = new_parser['mb_size']\n","iteration         = new_parser['iteration']\n","iteration_burn_in = new_parser['iteration_burn_in']\n","\n","keep_prob         = new_parser['keep_prob']\n","lr_train          = new_parser['lr_train']\n","\n","alpha             = new_parser['alpha']\n","beta              = new_parser['beta']\n","gamma             = new_parser['gamma']\n","\n","# SAVE HYPERPARAMETERS\n","log_name = file_path + '/hyperparameters_log.txt'\n","save_logging(new_parser, log_name)"]},{"cell_type":"markdown","metadata":{"id":"KJv6R2tNk5tF"},"source":["### 3. Split Dataset into Train/Valid/Test Sets"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1676320922356,"user":{"displayName":"Kevin Yu","userId":"12965521010022133262"},"user_tz":360},"id":"Ervy1lB8k5tF"},"outputs":[],"source":["indecies_mesa = pd.read_csv('/content/drive/My Drive/Longitudinal Risk Prediction/Dynamic-DeepHit LRPP/random_index_select.csv')\n","indecies_mesa = np.array(indecies_mesa['study_index'])\n","\n","df_index = list(range(len(data)))\n","\n","def train_test_split_index(df, indecies):\n","    tr_data = df[indecies == False]\n","    te_data = df[indecies]\n","    \n","    return tr_data, te_data\n","\n","### List Split Option\n","split_option = 'MESA'\n","\n","if split_option == 'MESA':\n","\n","    ### Select MESA as Testing set \n","    tr_data, te_data = train_test_split_index(data, indecies_mesa)\n","    tr_data_raw, te_data_raw = train_test_split_index(df_raw, indecies_mesa)\n","    tr_df_index, te_df_index = train_test_split_index(np.array(df_index), indecies_mesa)\n","    tr_data_mi, te_data_mi = train_test_split_index(data_mi, indecies_mesa)\n","    tr_time, te_time = train_test_split_index(time, indecies_mesa)\n","    tr_label, te_label = train_test_split_index(label, indecies_mesa)\n","    tr_mask1, te_mask1 = train_test_split_index(mask1, indecies_mesa)\n","    tr_mask2, te_mask2 = train_test_split_index(mask2, indecies_mesa)\n","    tr_mask3, te_mask3 = train_test_split_index(mask3, indecies_mesa) \n","else:\n","    ### TRAINING-TESTING SPLIT\n","    (tr_data, te_data, tr_data_raw, te_data_raw, tr_df_index, te_df_index, tr_data_mi, te_data_mi, tr_time, te_time, tr_label, te_label, \n","    tr_mask1,te_mask1, tr_mask2,te_mask2, tr_mask3,te_mask3) = train_test_split(data, df_raw, df_index, data_mi, time, label, mask1, mask2, mask3, test_size=0.2, random_state=seed, stratify = label) \n","\n","(tr_data, va_data, tr_data_mi, va_data_mi, tr_time, va_time, tr_label, va_label, \n"," tr_mask1,va_mask1, tr_mask2,va_mask2, tr_mask3,va_mask3) = train_test_split(tr_data, tr_data_mi, tr_time, tr_label, tr_mask1, tr_mask2, tr_mask3, test_size= 0.25, random_state=seed, stratify = tr_label) \n","\n","if boost_mode == 'ON':\n","    tr_data, tr_data_mi, tr_time, tr_label, tr_mask1, tr_mask2, tr_mask3 = f_get_boosted_trainset(tr_data, tr_data_mi, tr_time, tr_label, tr_mask1, tr_mask2, tr_mask3)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1676320922356,"user":{"displayName":"Kevin Yu","userId":"12965521010022133262"},"user_tz":360},"id":"hEHjDe_6_H0q"},"outputs":[],"source":["# get validation AUCs\n","validation_data = True\n","if validation_data == True:\n","  te_data = va_data\n","  te_time = va_time\n","  te_label = va_label\n","  te_mask1 = va_mask1\n","  te_mask2 = va_mask2\n","  te_mask3 = va_mask3\n","\n","training_data = False\n","if training_data == True:\n","  te_data = tr_data\n","  te_time = tr_time\n","  te_label = tr_label\n","  te_mask1 = tr_mask1\n","  te_mask2 = tr_mask2\n","  te_mask3 = tr_mask3"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1676320922357,"user":{"displayName":"Kevin Yu","userId":"12965521010022133262"},"user_tz":360},"id":"vc8_0zkFk5tG"},"outputs":[],"source":["##### CREATE DYNAMIC-DEEPHIT NETWORK\n","tf.reset_default_graph()\n","\n","config = tf.ConfigProto()\n","config.gpu_options.allow_growth = True\n","sess = tf.Session(config=config)\n","\n","model = Model_Longitudinal_Attention(sess, \"Dyanmic-DeepHit\", input_dims, network_settings)\n","saver = tf.train.Saver()\n","\n","sess.run(tf.global_variables_initializer())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kfrt_HfIk5tG","executionInfo":{"status":"aborted","timestamp":1676320922357,"user_tz":360,"elapsed":10,"user":{"displayName":"Kevin Yu","userId":"12965521010022133262"}}},"outputs":[],"source":["### TRAINING - BURN-IN\n","if burn_in_mode == 'ON':\n","    print( \"BURN-IN TRAINING ...\")\n","    for itr in range(iteration_burn_in):\n","        x_mb, x_mi_mb, k_mb, t_mb, m1_mb, m2_mb, m3_mb = f_get_minibatch(mb_size, tr_data, tr_data_mi, tr_label, tr_time, tr_mask1, tr_mask2, tr_mask3)\n","        DATA = (x_mb, k_mb, t_mb)\n","        MISSING = (x_mi_mb)\n","\n","        _, loss_curr = model.train_burn_in(DATA, MISSING, keep_prob, lr_train)\n","\n","        if (itr+1)%1000 == 0:\n","            print('itr: {:04d} | loss: {:.4f}'.format(itr+1, loss_curr))\n","\n","\n","### TRAINING - MAIN\n","print( \"MAIN TRAINING ...\")\n","min_valid = 0.5\n","\n","for itr in range(iteration):\n","    x_mb, x_mi_mb, k_mb, t_mb, m1_mb, m2_mb, m3_mb = f_get_minibatch(mb_size, tr_data, tr_data_mi, tr_label, tr_time, tr_mask1, tr_mask2, tr_mask3)\n","    DATA = (x_mb, k_mb, t_mb)\n","    MASK = (m1_mb, m2_mb, m3_mb)\n","    MISSING = (x_mi_mb)\n","    PARAMETERS = (alpha, beta, gamma)\n","\n","    _, loss_curr = model.train(DATA, MASK, MISSING, PARAMETERS, keep_prob, lr_train)\n","\n","    if (itr+1)%1000 == 0:\n","        print('itr: {:04d} | loss: {:.4f}'.format(itr+1, loss_curr))\n","\n","    ### VALIDATION  (based on average C-index of our interest)\n","    if (itr+1)%1000 == 0:        \n","        risk_all = f_get_risk_predictions(sess, model, va_data, va_data_mi, pred_time, eval_time)\n","        \n","        for p, p_time in enumerate(pred_time):\n","            pred_horizon = int(p_time)\n","            val_result1 = np.zeros([num_Event, len(eval_time)])\n","            \n","            for t, t_time in enumerate(eval_time):                \n","                eval_horizon = int(t_time) + pred_horizon\n","                for k in range(num_Event):\n","                    val_result1[k, t] = c_index(risk_all[k][:, p, t], va_time, (va_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n","            \n","            if p == 0:\n","                val_final1 = val_result1\n","            else:\n","                val_final1 = np.append(val_final1, val_result1, axis=0)\n","\n","        tmp_valid = np.mean(val_final1)\n","\n","        if tmp_valid >  min_valid:\n","            min_valid = tmp_valid\n","            saver.save(sess, file_path + '/model')\n","            print( 'updated.... average c-index = ' + str('%.4f' %(tmp_valid)))"]},{"cell_type":"markdown","metadata":{"id":"eGgWsP_Nk5tG"},"source":["### 5. Test the Trained Network"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4_-l3W4VKV8m","executionInfo":{"status":"aborted","timestamp":1676320922357,"user_tz":360,"elapsed":10,"user":{"displayName":"Kevin Yu","userId":"12965521010022133262"}}},"outputs":[],"source":["saver.restore(sess, file_path + '/model')\n","#saver.restore(sess, '/content/drive/My Drive/Longitudinal Risk Prediction/Dynamic-DeepHit LRPP/PBC2/model')\n","\n","risk_all = f_get_risk_predictions(sess, model, te_data, te_data_mi, pred_time, eval_time)\n","\n","for p, p_time in enumerate(pred_time):\n","    pred_horizon = int(p_time)\n","    result1, result2 = np.zeros([num_Event, len(eval_time)]), np.zeros([num_Event, len(eval_time)])\n","\n","    for t, t_time in enumerate(eval_time):                \n","        eval_horizon = int(t_time) + pred_horizon\n","        for k in range(num_Event):\n","            result1[k, t] = c_index(risk_all[k][:, p, t], te_time, (te_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n","            result2[k, t] = brier_score(risk_all[k][:, p, t], te_time, (te_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n","   \n","    if p == 0:\n","        final1, final2 = result1, result2\n","    else:\n","        final1, final2 = np.append(final1, result1, axis=0), np.append(final2, result2, axis=0)\n","        \n","        \n","row_header = []\n","for p_time in pred_time:\n","    for k in range(num_Event):\n","        row_header.append('pred_time {}: event_{}'.format(p_time,k+1))\n","            \n","col_header = []\n","for t_time in eval_time:\n","    col_header.append('eval_time {}'.format(t_time))\n","\n","# c-index result\n","df1 = pd.DataFrame(final1, index = row_header, columns=col_header)\n","\n","# brier-score result\n","df2 = pd.DataFrame(final2, index = row_header, columns=col_header)\n","\n","### PRINT RESULTS\n","print('========================================================')\n","print('--------------------------------------------------------')\n","print('- C-INDEX: ')\n","print(df1)\n","print('--------------------------------------------------------')\n","print('- BRIER-SCORE: ')\n","print(df2)\n","print('========================================================')"]},{"cell_type":"markdown","metadata":{"id":"f9heeT87k5tH"},"source":["### AUC Calculation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HqOwFmcsk5tH","executionInfo":{"status":"aborted","timestamp":1676320922357,"user_tz":360,"elapsed":10,"user":{"displayName":"Kevin Yu","userId":"12965521010022133262"}}},"outputs":[],"source":["def prediction_df(risk_all, event):\n","    \"\"\"\n","    Gets the individual predictions from risk_all\n","    event = 1\n","    \"\"\"\n","    risk_list = risk_all[event-1].tolist()\n","\n","    risk_list = [risk for risk_sublist in risk_list for risk in risk_sublist]\n","    risk_list = [risk for risk_sublist in risk_list for risk in risk_sublist]\n","\n","    return risk_list\n","\n","def prediction_pred_values(pred_t, eval_t , event_n, risk):\n","    \"\"\"Gets the predication values and true labels for AUC CI calculation\"\"\"\n","    \n","    global true_list\n","    \n","    time_horizon = pred_t + eval_t \n","    true = (te_time <= time_horizon) * (te_label == event_n).astype(int).tolist()\n","    \n","    true_list = [true_i for true_sublist in true for true_i in true_sublist]\n","    true_list = [true_i for true_sublist in true for true_i in true_sublist]\n","\n","    pred_df = pd.DataFrame({'ddh_risk' : risk, \n","                            'true_labels' : true_list})\n","    \n","    return pred_df\n","\n","def prediction_ROC(pred_t, eval_t , event_n, df):\n","    \"\"\"Calculates the AUC at each prediction time and prediction time\"\"\"\n","    \n","    global true \n","    \n","    time_horizon = pred_t + eval_t \n","    true = (te_time <= 15) * (te_label == 1).astype(int)\n","\n","    fpr, tpr, thresh = roc_curve(true.tolist(), df.loc[(df.pred_time == pred_t) & (df.eval_time == eval_t), 'value'].tolist())\n","            \n","    return fpr, tpr, thresh"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AX3fiRKek5tH","executionInfo":{"status":"aborted","timestamp":1676320922358,"user_tz":360,"elapsed":11,"user":{"displayName":"Kevin Yu","userId":"12965521010022133262"}}},"outputs":[],"source":["# Get individual risks for AUC calculation (DDH)\n","risk_df = prediction_df(risk_all, 1)\n","pred_df = prediction_pred_values(8, 10, 1, risk_df)\n","round(roc_auc_score(pred_df['true_labels'], pred_df['ddh_risk']), 3)"]},{"cell_type":"code","source":["pred_df.to_csv('/content/drive/My Drive/Longitudinal Risk Prediction/Dynamic-DeepHit LRPP/pred_df_gender.csv', index = False)"],"metadata":{"id":"iNJHPdiorrxA","executionInfo":{"status":"aborted","timestamp":1676320922358,"user_tz":360,"elapsed":11,"user":{"displayName":"Kevin Yu","userId":"12965521010022133262"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U3z82Fgpn4eN","executionInfo":{"status":"aborted","timestamp":1676320922358,"user_tz":360,"elapsed":46877,"user":{"displayName":"Kevin Yu","userId":"12965521010022133262"}}},"outputs":[],"source":[" pred_df.ddh_risk.hist()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f5NquyALk5tI","executionInfo":{"status":"aborted","timestamp":1676320922358,"user_tz":360,"elapsed":46876,"user":{"displayName":"Kevin Yu","userId":"12965521010022133262"}}},"outputs":[],"source":["# output risk \n","#pred_df.to_csv('/content/drive/My Drive/Longitudinal Risk Prediction/Dynamic-DeepHit LRPP/DDH_pred_df_current.csv', index = False)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}